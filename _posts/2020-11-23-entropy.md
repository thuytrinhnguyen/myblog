---
layout: post
title:  "Entropy - Information Gain Theory"
date:   2020-11-23 11:46
categories: 
tags: decision-tree information-theory entropy
---

> Entropy is a basic concept that you might have come across while studying about Decision Tree or machine learning cross-entropy loss function. This post will address the two interpretations of entropy to help you understand the concept better.

<!--more-->

<mark><b>Highlights</b></mark> 

{: class="table-of-content"}
* TOC
{:toc}


## Introduction

Entropy is a concept of the information theory introduced by Shannon Claude. Today I will discuss two interpretations of this concept, entropy as (1) a measure of uncertainty and as (2) a measure of useful information.

The formula for entropy is:

$$
H(p) = -\sum_{i} p_i \times log(p_i)
$$
 
where $$i$$ is the total number of possible events and $$p_i$$ is the probability of each event.

Since $$log(\frac{1}{p_i}) = - log(p_i)$$, another way to phrase this: $$H(p) = \sum_{i} p_i \times log(\frac{1}{p_i})$$.

## Measure of uncertainty

There are many explanations around entropy, among which entropy as a measure of uncertainty is the most popular idea. 

Think of entropy as __How many yes-no question needed on average to reach a decision?__

### Two-state weather with equal likelihood

{% figure caption: "Fig. 1. Two-state weather with 50% probability each" %}
![]({{'/assets/images/entropy-2-50.png'}})
{% endfigure %}

For example, if there are only 2 states of the weather _Sunny_ and _Rainy_, you can just ask one question _"Is it sunny tomorrow?"_, whether the answer is yes or no, you will know the weather tomorrow in one question. The entropy of this scenario is:

$$H_{2\_states\_equal} = - 0.5 \times log(0.5) - 0.5 \times log(0.5) = 1 $$

### Eight-state weather with equal likelihood

What if there are 8 equally-likely states? You will probably need 3 questions to come to the final answer demonstrated in Fig. 2. 

* __Question 1.__ Is the weather in [the upper 4 states]?

     Yes $$\rightarrow $$ Eliminate 4 lower states, 4 upper states remaining.

* __Question 2.__ Is the weather in [the right hand side 2 states]?

     No $$\rightarrow $$ Eliminate 2 right hand side states, 2 states (_Sunny_ & _Partially Cloudy_) remaining.

* __Question 3.__ Is the weather _Sunny_?

     Yes $$\rightarrow $$ Eliminate _Partially Cloudy_, the final answer is Sunny.

As expected, the entropy is this scenario is:  $$H_{8\_states\_equal} = - 8 \times 0.125 \times log(0.125) = 3 $$

{% figure caption: "Fig. 2. Eight-state weather with 12.5% probability each"%}
![]({{'/assets/images/entropy-8-3.png'}})
{% endfigure %}

From these examples, we reach two conclusions:

> __<mark>(1)</mark> Entropy is the number of yes/no question _on average_ to classify the data.__ 

> __<mark>(2)</mark> The higher number of classifiers, the more random the data, hence, the higher the entropy.__

But, is the number of classifiers the only indicator of uncertainty in a dataset? So far we have only considered cases of events with equal probabilities, let's have some examples of classes with different likelihood to compare their entropy.

### Events with different probabilities

In a tropical country, the weather is mostly sunny (75% of the time) and rainy sometimes (25% of the time). 

{% figure caption: "Fig. 3. Two-state weather with 75% probability sunny and 25% rainy" %}
![]({{'/assets/images/entropy-2-75.png'}})
{% endfigure %}

$$H_{2\_states\_skewed} = - 0.75 \times log(0.75) - 0.25 \times log(0.25)  = 0.81 $$

This time, when calculating entropy (the number of questions), we also need to consider the weights of each event. The entropy in this case is $$0.81$$, which is less than when Sunny and Rainy weather are equally likely. To confirm whether it is true that we can ask fewer questions when the probabilities of events are not equal, let's take another example of a 8-state weather of another tropical country.  

{% figure caption: "Fig. 4. Eight-state weather with mixed probabilities"%}
![]({{'/assets/images/entropy-8-35.png'}})
{% endfigure %}

Let's calculate the entropy of this eight-state weather with mixed probabilities:

$$
\begin{aligned}
H_{8\_states\_skewed\_35} 
&= - 2 \times 0.35 \times log(0.35) - 2 \times 0.1 \times log(0.1) \\
& \; \; \; - 2 \times 0.04 \times log(0.04) - 2 \times 0.01 \times log(0.01) \\
&= 2.23 
\end{aligned}
$$

What if the weather likelihood is even more skewed? Let's calculate the entropy for this distribution: 45% - 25% - 10% - 10% - 4% - 4% - 1% - 1%.

$$
\begin{aligned}
H_{8\_states\_skewed\_45} 
&= - 0.45 \times log(0.45) - 0.25 \times log(0.25) - 2 \times 0.1 \times log(0.1) \\
&\; \; \; - 2 \times 0.04 \times log(0.04) - 2 \times 0.01 \times log(0.01) \\
&= 2.19
\end{aligned}
$$

Comparison: $$H_{8\_states\_equal} > H_{8\_states\_skewed\_35} > H_{8\_states\_skewed\_45}$$

Therefore, we reach the third conclusion:

> __<mark>(3)</mark> The more skewed the likelihood of events, the less uncertainty of the data, hence, the lower the entropy.__

By testing the entropy against different situations, we know that conclusion (3) is an accurate statement. However, the intuition of entropy has not been channelled fully. We can look at a related term that can enhance our knowledge of entropy.

## Measure of useful information (Information gain)

A related term with entropy is information. While entropy is the __uncertainty before__ decision-making, information is the __additional knowledge after__ decision-making. 

### Relationship between Entropy and Information

Take an extreme example of weather in a desert which is sunny all year round $$(p_{Sunny} = 1)$$. What is the entropy in this case?

$$H_{sunny\_100} = - log(1) - 0 = 0$$

Comparison: $$H_{2\_states\_equal} > H_{2\_states\_skewed} > H_{sunny\_100}$$

When the weather is always sunny, we are 100% certain $$H_{sunny\_100} = 0$$. However, since the weather is always predictable, there is no value in having weather forecast or asking any yes/no questions. Hence, the information gain is zero. 

When it is 50% sunny and 50% rainy, by telling that the weather will be rainy tomorrow, our uncertainty reduces by half. Hence, the information we get from knowing the weather will be rainy is: $$log(\frac{1}{0.5})= $$ 2 bits of information. Similarly, if the weather station forecasts that tomorrow will be sunny, the amount of information we get is also 2 bits as the two states are equally likely to happen. Taking into account the likelihood of both events, we have the _average_ information gain in this scenario: $$0.5 \times 2 + 0.5 \times 2 = 1$$, this is our entropy ($$H_{2\_states\_equal} = 1$$)! Therefore, the two interpretations are both accurate, we can look at entropy from different perspectives. 

Think about it for a second, you should see why it totally makes sense. If the case is predictable (i.e. there is no randomness), there is no new information in telling us what we are already certain of. On the contrary, if the case has many states of different probabilities, we are not sure about the outcome, hence, there is some information gain in telling us what will happen. This takes us to our fourth conclusion:

> __<mark>(4)</mark> The higher the randomness (higher entropy), the higher the information gain.__

But why do we need to know about information if it is just another way to intuitively think about entropy?

### Why information gain matters

We concern about information gain because we wish to measure the effectiveness of communication: __How to send useful information using the least number of bits?__ 

Remember that we have calculated $$H_{2\_state\_equal} = 1$$, this is 1 bit of __useful information__. No matter how we convey the message, only 1 bit is useful in this case (e.g. sending 0-1 signal with 1 being sunny weather and 0 otherwise). In this example, 1 bit is the optimal number of bit we should send to have the most effective communication.

{% figure caption: "Eight-state weather with 3-bit assignment"%}
![]({{'/assets/images/entropy-8-3b.png'}})
{% endfigure %}

For a 8-state weather with skewed likelihood, $$H_{8\_states\_skewed\_35} = 2.23$$. This means no matter how we deliver the message, on average only 2.23 useful bits are received.

If we use 3 bits to denote each state of the weather, the number of bits we send out on average is:

$$H= 2 \times 0.35 \times 3 + 2 \times 0.1 \times 3 + 2 \times 0.04 \times 3 + 2 \times 0.01 \times 3 = 3$$

The result is suboptimal as we are $$0.77$$ bit over the optimal value. We can try adjusting the number of bits assigned to each event and look for a combination whose total is the closet to number of useful bits.

{% figure caption: "Eight-state weather with updated bit assignments"%}
![]({{'/assets/images/entropy-8-5.png'}})
{% endfigure %}

$$H = 2\times 0.35 \times 2 + 2 \times 0.1 \times 3 + 2 \times 0.04 \times 4 + 2 \times 0.01 \times 5 = 2.42 $$

We are becoming more effective by assigning fewer bits on the low-value events and __more bits on the rare events__, therefore, we will not waste lots of bits to events that happen most of the time as expected.
 
## Summary

### Applications

Entropy is used as a splitting criteria for Decision Tree. Most commonly, people use Decision Tree for classification problems whose ultimate goal is to have pure sets in the end. Using entropy, we can measure the impurity in the set, split the data so that after each step, the data is more defined until the set has a zero entropy.

 Another widely used term related to entropy is cross-entropy. Instead of measuring the uncertainty within one distribution like entropy, cross-entropy compares two probability distributions. It is a popular loss function in Machine learning to compare model prediction and ground truth values. 
 
### Key Takeaways 

Understanding entropy gives you the intuitive thinking in various problems. The key takeaways from this post are:

* __Entropy measures uncertainty:__ 

    * Entropy is the average number of yes/no questions on you ask to classify the data. 
    
    * The more skewed the probabilities of events are, the higher uncertainty, hence, higher entropy.
    
* __Entropy measures information gain:__ 

    * When the event is predictable and be more likely to behave in one way, there is little information gain. 
    
    * Therefore, the higher entropy (uncertainty), the higher information gain.

I hope that this post on entropy has been helpful. Happy learning!

__Chloe's End Note__
> It takes me a full week to fully understand this seemingly straightforward concept. But it is such a relief to finally get this concept right so I can move on to Decision Tree smoothly. Also, I hope that the illustrations are joyful to look at. I had a good time drawing them :)